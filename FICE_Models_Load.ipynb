{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mob2dr/AI-2021/blob/main/FICE_Models_Load.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0QJV0q9M0Fb"
      },
      "outputs": [],
      "source": [
        "# ! cd /content/drive/MyDrive/FICE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/MartinPernus/FICE.git"
      ],
      "metadata": {
        "id": "6SfPSUZiNt7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !mv /content/FICE /content/drive/MyDrive/FICE"
      ],
      "metadata": {
        "id": "Ipr4YRJ4NvOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! /content/FICE/download.sh"
      ],
      "metadata": {
        "id": "5u0EqGd1N3OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! mkdir /content/drive/MyDrive/TGIEFF/FICE/models/densepose/checkpoints"
      ],
      "metadata": {
        "id": "r7xaufcFQhKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! cp /content/checkpoints/densepose.pkl /content/drive/MyDrive/TGIEFF/FICE/models/densepose/checkpoints/densepose.pkl"
      ],
      "metadata": {
        "id": "Usx4Dg3FOHGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! apt-get install ninja-build"
      ],
      "metadata": {
        "id": "50uX7JAZaB3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install -r /content/drive/MyDrive/TGIEFF/FICE/requirements.txt"
      ],
      "metadata": {
        "id": "DNvp4ZFjXf16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7IrMsjtW1VmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.argv=['']\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/TGIEFF/FICE/')\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from easydict import EasyDict as edict\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from utils import *\n",
        "import trainer\n",
        "from trainer import Trainer\n",
        "import torch\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--batch_size', type=int, default=6)\n",
        "parser.add_argument('--gpu', type=int, default=0)\n",
        "parser.add_argument('--description', type=str, default=None)\n",
        "parser.add_argument('--input_dir', type=str, default=None)\n",
        "parser.add_argument('--clip_model', type=str, default='RN50x4')\n",
        "args = parser.parse_args()\n",
        "\n",
        "description = args.description or 'red t-shirt'\n",
        "input_dir = args.input_dir or 'imgs/input'\n",
        "\n",
        "device = 'cuda:%d' % args.gpu\n",
        "opt = edict(bs=args.clip_model)\n",
        "trainer = Trainer(opt)\n",
        "trainer.to(device)\n",
        "imgs = load_imgdir(input_dir).to(device)\n",
        "\n",
        "outdir = 'output'\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "def get_filename(sentence):\n",
        "    sent_temp = sentence.replace('/', '-')  # avoid difficulties with creation of new folder\n",
        "    filename = os.path.join(outdir, sent_temp + '.jpg')\n",
        "    return filename\n",
        "\n",
        "def save_comparison(imgs_final, filename):\n",
        "    imgs_compare = torch.cat((imgs, imgs_final), dim=0).cpu()\n",
        "    save_image(imgs_compare, filename, nrow=len(imgs))\n",
        "\n",
        "print('Target description:', description)\n",
        "filename = get_filename(description)\n",
        "\n",
        "trainer.init(imgs, description)\n",
        "imgs_final = trainer.process()\n",
        "save_comparison(imgs_final, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2VLfyN7fqoa",
        "outputId": "0cb4602b-b98e-4173-b776-e31a7746e922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target description: red t-shirt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2001 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up PyTorch plugin \"bias_act_plugin\"... Failed!\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/TGIEFF/FICE/models/stylegan/torch_utils/ops/bias_act.py:50: UserWarning: Failed to build CUDA kernels for bias_act. Falling back to slow reference implementation. Details:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/TGIEFF/FICE/models/stylegan/torch_utils/ops/bias_act.py\", line 48, in _init\n",
            "    _plugin = custom_ops.get_plugin('bias_act_plugin', sources=sources, extra_cuda_cflags=['--use_fast_math'])\n",
            "  File \"/content/drive/MyDrive/TGIEFF/FICE/models/stylegan/torch_utils/custom_ops.py\", line 110, in get_plugin\n",
            "    torch.utils.cpp_extension.load(name=module_name, verbose=verbose_build, sources=sources, **build_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 1284, in load\n",
            "    return _jit_compile(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 1509, in _jit_compile\n",
            "    _write_ninja_file_and_build_library(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 1624, in _write_ninja_file_and_build_library\n",
            "    _run_ninja_build(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 1893, in _run_ninja_build\n",
            "    subprocess.run(\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 505, in run\n",
            "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1141, in communicate\n",
            "    stdout = self.stdout.read()\n",
            "KeyboardInterrupt\n",
            "\n",
            "  warnings.warn('Failed to build CUDA kernels for bias_act. Falling back to slow reference implementation. Details:\\n\\n' + traceback.format_exc())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "8.517: 100%|██████████| 2001/2001 [07:16<00:00,  4.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = load_imgdir(input_dir).to(device)\n",
        "\n",
        "outdir = 'output'\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "def get_filename(sentence):\n",
        "    sent_temp = sentence.replace('/', '-')  # avoid difficulties with creation of new folder\n",
        "    filename = os.path.join(outdir, sent_temp + '.jpg')\n",
        "    return filename\n",
        "\n",
        "def save_comparison(imgs_final, filename):\n",
        "    imgs_compare = torch.cat((imgs, imgs_final), dim=0).cpu()\n",
        "    save_image(imgs_compare, filename, nrow=len(imgs))\n",
        "\n",
        "print('Target description:', description)\n",
        "filename = get_filename(description)\n",
        "\n",
        "trainer.init(imgs, description)\n",
        "imgs_final = trainer.process()\n",
        "save_comparison(imgs_final, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb0TF2sVpmgD",
        "outputId": "45b0715e-05b7-4909-a1c6-99f79a1587cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target description: long sleeve silk crepe de chine shirt featuring graphic pattern printed in tones of blue\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.627: 100%|██████████| 501/501 [01:32<00:00,  5.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.chdir('/content/drive/MyDrive/TGIEFF/FICE/')\n",
        "# import models.segmentation\n",
        "# from models.segmentation import model\n",
        "# from models.segmentation.model import *\n",
        "# import utils"
      ],
      "metadata": {
        "id": "PuNi2hL-Q5Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s = SegModel()"
      ],
      "metadata": {
        "id": "X20Uy_YJZJNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import models.densepose\n",
        "# from models.densepose import model\n",
        "# from models.densepose.model import *\n",
        "# import utils"
      ],
      "metadata": {
        "id": "Xff-7nIAeKdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dense = DenseNet()"
      ],
      "metadata": {
        "id": "jduIRHweehrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/TGIEFF/FICE/')\n",
        "import models.Clip\n",
        "from models.Clip import model\n",
        "from models.Clip.model import *\n",
        "import utils"
      ],
      "metadata": {
        "id": "nRYDgTYbg_4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model = CLIP()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s4gIdKIJy47",
        "outputId": "02aaa472-6c15-44d4-e827-1fb3d22a1dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 402M/402M [00:03<00:00, 133MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/TGIEFF/FICE/')\n",
        "import models.e4e\n",
        "from models.e4e import model\n",
        "from models.e4e.model import *\n",
        "import utils"
      ],
      "metadata": {
        "id": "sm0SZKHTKjHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04e93bc-4251-481b-9241-14f35be12765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing StyleGAN modules\n",
            "StyleGAN modules imported\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e4e_model = load_e4e()"
      ],
      "metadata": {
        "id": "ElouZeXpaP9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torchvision import transforms\n",
        "# from PIL import Image\n",
        "\n",
        "# # Load and prepare the input image\n",
        "# image = Image.open('/content/3.jpg')\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((e4e_model.img_size, e4e_model.img_size)),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "# image_tensor = transform(image).unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# # Pass the image through the model\n",
        "# with torch.no_grad():\n",
        "#     w = e4e_model.forward(image_tensor)\n",
        "\n",
        "# # `w` now contains the estimated style vector for the input image\n",
        "# w"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWBQKS9ddu1P",
        "outputId": "838dcd69-6440-484d-ac9b-e390dd7c4eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.3702,  0.1861,  0.4439,  ..., -0.6251, -0.0117,  0.7760],\n",
              "         [ 0.5393,  0.2155,  0.4869,  ..., -0.4250,  0.3123,  1.0436],\n",
              "         [ 0.6347,  0.4004,  0.5797,  ..., -1.6492, -0.1219,  0.6234],\n",
              "         ...,\n",
              "         [-0.6122,  0.8121, -0.1781,  ..., -0.5877,  1.3511,  0.3540],\n",
              "         [ 0.5551, -0.1335,  1.3181,  ..., -1.3037, -0.1700,  0.3761],\n",
              "         [ 0.8826, -0.2855,  0.8867,  ...,  0.2527, -0.2075,  0.2570]]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jbhfGAczY5OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fs8sXKdvazsd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}